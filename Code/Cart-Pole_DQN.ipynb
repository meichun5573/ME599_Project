{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76538cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import logger, spaces\n",
    "from gym.envs.classic_control import utils\n",
    "from gym.error import DependencyNotInstalled\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Cartpole environment and physical model\n",
    "class CartPoleEnv(gym.Env[np.ndarray, Union[int, np.ndarray]]):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 50,\n",
    "    }\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        \n",
    "        # Physical Variables\n",
    "        self.gravity = 9.81    \n",
    "        self.masscart = 1.0   \n",
    "        self.masspole = 0.5   \n",
    "        self.length = 0.5      # actually half the pole's length\n",
    "        self.force_mag = 20.0  # step force, +1 or -1\n",
    "        self.tau = 0.02        # timeframe(dt), seconds between state updates\n",
    "        \n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.kinematics_integrator = \"rk4\"\n",
    "\n",
    "        self.x_threshold = 10\n",
    "\n",
    "        high = np.array(\n",
    "            [\n",
    "                self.x_threshold * 2,\n",
    "                np.finfo(np.float32).max,\n",
    "                np.finfo(np.float32).max,\n",
    "                np.finfo(np.float32).max,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        \n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Screen variables\n",
    "        self.screen_width = 800\n",
    "        self.screen_height = 600\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.isopen = True\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "        \n",
    "        # ODE, RK4, euler and semi-euler methods\n",
    "        def ode(state, force):\n",
    "            x, x_dot, theta, theta_dot = self.state\n",
    "            \n",
    "            costheta = math.cos(theta)\n",
    "            sintheta = math.sin(theta)\n",
    "\n",
    "            temp = (\n",
    "                force + self.polemass_length * theta_dot**2 * sintheta\n",
    "            ) / self.total_mass\n",
    "            thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "                self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
    "            )\n",
    "            xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "            \n",
    "            state_dot = x_dot, xacc, theta_dot, thetaacc\n",
    "            return state_dot\n",
    "        \n",
    "        def state_toArray(state):\n",
    "            x, x_dot, theta, theta_dot = state\n",
    "            state_dot = np.zeros(4)\n",
    "            state_dot[0] = x\n",
    "            state_dot[1] = x_dot\n",
    "            state_dot[2] = theta\n",
    "            state_dot[3] = theta_dot\n",
    "            return state_dot\n",
    "            \n",
    "        def array_toState(arr):\n",
    "            return arr[0], arr[1], arr[2], arr[3]\n",
    "            \n",
    "        def euler_state(state, force):\n",
    "            x, __ , theta, __ = state\n",
    "            x_dot, xacc, theta_dot, thetaacc = ode(state, force)\n",
    "\n",
    "            x = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            \n",
    "            new_state = x, x_dot, theta, theta_dot\n",
    "            return new_state\n",
    "        \n",
    "        def rk4_state(state, force):\n",
    "            dt = self.tau\n",
    "            \n",
    "            k1 = dt * state_toArray(ode(state, force))\n",
    "            k2 = dt * state_toArray(ode(state + k1 / 2, force))\n",
    "            k3 = dt * state_toArray(ode(state + k2 / 2, force))\n",
    "            k4 = dt * state_toArray(ode(state + k3, force))\n",
    "            \n",
    "            state_arr = state_toArray(state)\n",
    "            result_arr = state_arr + ((k1 + 2 * k2 + 2 * k3 + k4) / 6)   \n",
    "            \n",
    "            return array_toState(result_arr)\n",
    "        \n",
    "        def semi_state(state,force):\n",
    "            x, __ , theta, __ = state\n",
    "            x_dot, xacc, theta_dot, thetaacc = ode(state, force)\n",
    "\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            \n",
    "            new_state = x, x_dot, theta, theta_dot\n",
    "            return new_state\n",
    "\n",
    "        # Call state function from above\n",
    "        if self.kinematics_integrator == \"euler\":\n",
    "            self.state = euler_state(self.state, force)\n",
    "        elif self.kinematics_integrator == \"rk4\":\n",
    "            self.state = rk4_state(self.state, force)\n",
    "        else:\n",
    "            self.state = semi_state(self.state, force)\n",
    "\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        \n",
    "        ## Terminated Condition\n",
    "        # Case 1. Out of bound = Terminated\n",
    "        terminated = bool(\n",
    "            x < -self.x_threshold\n",
    "            or x > self.x_threshold\n",
    "        )\n",
    "        \n",
    "        # Case 2. Reach the goal = Finished\n",
    "        finished = goal_check(self.state)\n",
    "        goal_reward = 15000\n",
    "        \n",
    "        if not terminated:\n",
    "            \n",
    "            if finished:      # Finished, Not Terminated\n",
    "                reward = goal_reward\n",
    "                terminated = True\n",
    "                self.steps_beyond_terminated = 0\n",
    "                \n",
    "            else:             # Not Finished, Not Terminate, Normal case\n",
    "                reward = reward_function(self.state, self.masspole, self.gravity,\n",
    "                                         self.length, self.x_threshold)\n",
    "                \n",
    "        else:\n",
    "            if self.steps_beyond_terminated is None:\n",
    "                self.steps_beyond_terminated = 0\n",
    "                \n",
    "                if finished:   # Finished And Terminated\n",
    "                    reward = goal_reward\n",
    "                    \n",
    "                else:          # Not Finished, but Terminated = Out of boundary\n",
    "                    reward = -10000\n",
    "                    \n",
    "            else:\n",
    "                if self.steps_beyond_terminated == 0:\n",
    "                    logger.warn(\n",
    "                        \"You are calling 'step()' even though this \"\n",
    "                        \"environment has already returned terminated = True. You \"\n",
    "                        \"should always call 'reset()' once you receive 'terminated = \"\n",
    "                        \"True' -- any further steps are undefined behavior.\"\n",
    "                    )\n",
    "                self.steps_beyond_terminated += 1\n",
    "                reward = 0.0\n",
    "                  \n",
    "            \n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), reward, terminated, False, {}\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: Optional[int] = None,\n",
    "        options: Optional[dict] = None,\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Initial State\n",
    "        self.state = (0, 0, math.pi, 0)\n",
    "        self.steps_beyond_terminated = None\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode is None:\n",
    "            gym.logger.warn(\n",
    "                \"You are calling render method without specifying any render mode. \"\n",
    "                \"You can specify the render_mode at initialization, \"\n",
    "                f'e.g. gym(\"{self.spec.id}\", render_mode=\"rgb_array\")'\n",
    "            )\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            import pygame\n",
    "            from pygame import gfxdraw\n",
    "        except ImportError:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gym[classic_control]`\"\n",
    "            )\n",
    "\n",
    "        if self.screen is None:\n",
    "            pygame.init()\n",
    "            if self.render_mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                self.screen = pygame.display.set_mode(\n",
    "                    (self.screen_width, self.screen_height)\n",
    "                )\n",
    "            else:  # mode == \"rgb_array\"\n",
    "                self.screen = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = self.screen_width / world_width\n",
    "        polewidth = 5.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 40.0\n",
    "        cartheight = 20.0\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x = self.state\n",
    "\n",
    "        self.surf = pygame.Surface((self.screen_width, self.screen_height))\n",
    "        self.surf.fill((255, 255, 255))\n",
    "\n",
    "        l, r, t, b = -cartwidth / 2, cartwidth / 2, cartheight / 2, -cartheight / 2\n",
    "        axleoffset = cartheight / 4.0\n",
    "        cartx = x[0] * scale + self.screen_width / 2.0  # MIDDLE OF CART\n",
    "        carty = 250                                     # TOP OF CART\n",
    "        cart_coords = [(l, b), (l, t), (r, t), (r, b)]\n",
    "        cart_coords = [(c[0] + cartx, c[1] + carty) for c in cart_coords]\n",
    "        gfxdraw.aapolygon(self.surf, cart_coords, (0, 0, 0))\n",
    "        gfxdraw.filled_polygon(self.surf, cart_coords, (0, 0, 0))\n",
    "\n",
    "        l, r, t, b = (\n",
    "            -polewidth / 2,\n",
    "            polewidth / 2,\n",
    "            polelen - polewidth / 2,\n",
    "            -polewidth / 2,\n",
    "        )\n",
    "\n",
    "        pole_coords = []\n",
    "        for coord in [(l, b), (l, t), (r, t), (r, b)]:\n",
    "            coord = pygame.math.Vector2(coord).rotate_rad(-x[2])\n",
    "            coord = (coord[0] + cartx, coord[1] + carty + axleoffset)\n",
    "            pole_coords.append(coord)\n",
    "        gfxdraw.aapolygon(self.surf, pole_coords, (202, 152, 101))\n",
    "        gfxdraw.filled_polygon(self.surf, pole_coords, (202, 152, 101))\n",
    "\n",
    "        gfxdraw.aacircle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "        gfxdraw.filled_circle(\n",
    "            self.surf,\n",
    "            int(cartx),\n",
    "            int(carty + axleoffset),\n",
    "            int(polewidth / 2),\n",
    "            (129, 132, 203),\n",
    "        )\n",
    "\n",
    "        gfxdraw.hline(self.surf, 0, self.screen_width, carty, (0, 0, 0))\n",
    "\n",
    "        self.surf = pygame.transform.flip(self.surf, False, True)\n",
    "        self.screen.blit(self.surf, (0, 0))\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "            pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            import pygame\n",
    "\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.isopen = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal check function\n",
    "def goal_check(state):\n",
    "    x, x_dot, theta, theta_dot = state\n",
    "    \n",
    "    theta_threshold = 5.0      # degree \n",
    "    theta_dot_threshold = 1.0  # omega\n",
    "    \n",
    "    theta_deg = math.degrees(theta)\n",
    "\n",
    "    # Return true if fulfill all requirements\n",
    "    if (abs(theta_deg) < theta_threshold\n",
    "       and abs(theta_dot) < theta_dot_threshold):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f1449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function\n",
    "def reward_function(state, m, g, h, x_max):\n",
    "    \n",
    "    x, x_dot, theta, theta_dot = state\n",
    "    theta = theta % (2.0*math.pi)\n",
    "    \n",
    "    # Energy Reward 0 to 10000\n",
    "    target_energy = 2.0 * m * g * h \n",
    "    rate_100 = 100 / target_energy\n",
    "    \n",
    "    inertia = (1.0 / 3.0) * m * (2.0 * h)**2\n",
    "    kinetic_energy = 0.5 * inertia * theta_dot**2\n",
    "    potential_energy = m * g * h * (1.0 + math.cos(theta))\n",
    "    current_energy = kinetic_energy + potential_energy\n",
    "    \n",
    "    energy_difference_100 = 100.0 - abs(100.0 - current_energy * rate_100   )\n",
    "    energy_reward = energy_difference_100 ** 2\n",
    "\n",
    "    \n",
    "    # Position Panalty 0 to 10000\n",
    "    warning_border = x_max - 3.0\n",
    "    position_panalty = 0.0\n",
    "    if (abs(x) > warning_border):\n",
    "        out_of_bound = (abs(x) - warning_border) / 3.0\n",
    "        position_panalty = (100.0 * out_of_bound) ** 2\n",
    "        \n",
    "    \n",
    "    reward = energy_reward - position_panalty\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33761063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export functions for csv and diagrams\n",
    "\n",
    "def ang2abs(theta):\n",
    "    theta = theta % (2 * math.pi)\n",
    "    return abs(theta - math.pi)\n",
    "\n",
    "# save the datapoints and plot in the end\n",
    "class DataPlotter:\n",
    "    def __init__(self, run):\n",
    "        self.min_reward = []\n",
    "        self.max_reward = []\n",
    "        self.explore_data = []\n",
    "        self.max_angle = []\n",
    "        self.scores = []\n",
    "        self.cur_reward = []\n",
    "        self.cur_angle = []\n",
    "        self.final_reward = []\n",
    "        self.runs = [i for i in range(run)]\n",
    "        self.f_name = \"a\"\n",
    "        \n",
    "    def add_reward_angle(self, reward, angle):\n",
    "        self.cur_reward.append(reward)\n",
    "        self.cur_angle.append(ang2abs(angle))\n",
    "        \n",
    "    def append(self, exp, score, rf):\n",
    "        \n",
    "        # append\n",
    "        self.min_reward.append(min(self.cur_reward))\n",
    "        self.max_reward.append(max(self.cur_reward))\n",
    "        self.max_angle.append(max(self.cur_angle))\n",
    "        self.explore_data.append(exp)\n",
    "        self.scores.append(score)\n",
    "        self.final_reward.append(rf)\n",
    "        \n",
    "        # clear\n",
    "        self.cur_reward.clear()\n",
    "        self.cur_angle.clear()\n",
    "        \n",
    "    def plot(self):\n",
    "\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "            \n",
    "        # graph 1\n",
    "        ax[0, 0].plot(self.runs, self.explore_data, color = 'blue')\n",
    "        ax[0, 0].set_ylabel('Percentage')\n",
    "        ax[0, 0].set_xlabel('Run')\n",
    "        ax[0, 0].set_title('Exploration rate')\n",
    "        \n",
    "        # graph 2\n",
    "        ax[0, 1].plot(self.runs, self.max_reward,  color = 'green')\n",
    "        ax[0, 1].set_ylabel('Reward')\n",
    "        ax[0, 1].set_xlabel('Run')\n",
    "        ax[0, 1].set_title('Max Reward')\n",
    "        \n",
    "        # graph 3\n",
    "        ax[1, 0].plot(self.runs, self.max_angle, color = 'red')\n",
    "        ax[1, 0].set_ylabel('Degree')\n",
    "        ax[1, 0].set_xlabel('Run')\n",
    "        ax[1, 0].set_title('Max Angle')\n",
    "        \n",
    "        # graph 4\n",
    "        ax[1, 1].plot(self.runs, self.scores, color = 'purple')\n",
    "        ax[1, 1].set_ylabel('Score')\n",
    "        ax[1, 1].set_xlabel('Run')\n",
    "        ax[1, 1].set_title('Score')\n",
    "        \n",
    "        # Adjust the spacing between subplots\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        figPath = os.path.join(self.f_name, \"result\")\n",
    "        plt.savefig(figPath)\n",
    "        \n",
    "    # create the folder\n",
    "    def folder(self):\n",
    "        self.f_name = create_folder_and_csv()\n",
    "        \n",
    "    # create the CSV file in the folder    \n",
    "    def toCSV(self):\n",
    "        filename = self.f_name\n",
    "        filename = os.path.join(filename, \"data.csv\")\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(['Reward', 'Exploration rate', 'Max Angle',\n",
    "                             'Scores', 'Final reward'])  # Write header row\n",
    "            for row in zip(self.max_reward, self.explore_data, self.max_angle,\n",
    "                           self.scores, self.final_reward):\n",
    "                writer.writerow(row)\n",
    "                \n",
    "def create_folder_and_csv():\n",
    "    # Get current time\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Create a new folder\n",
    "    folder_name = f\"result_{current_time}\"\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "    # Create a CSV file inside the folder\n",
    "    return folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e68414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Training Model\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "ENV_NAME = \"CartPole\"\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "\n",
    "class DQNSolver():\n",
    "\n",
    "    def __init__(self, observation_space, action_space, exploration_decay,\n",
    "                 batch_size, learing_rate, gamma):\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))\n",
    "        self.model.add(Dense(24, activation=\"relu\"))\n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))\n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=learing_rate))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, reward, state_next, terminal in batch:\n",
    "            q_update = reward\n",
    "            if not terminal:\n",
    "                q_update = (reward + self.gamma * np.amax(self.model.predict(state_next)[0]))\n",
    "            q_values = self.model.predict(state)\n",
    "            q_values[0][action] = q_update\n",
    "            self.model.fit(state, q_values, verbose=0)\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "        \n",
    "class CartPole():\n",
    "    def __init__ (self, run):\n",
    "        self.dp = DataPlotter(run)\n",
    "        \n",
    "    def train(self, exploration_decay, batch_size, learing_rate, gamma, iterr):\n",
    "        print(\"Training Started\")\n",
    "        env = CartPoleEnv()\n",
    "        observation_space = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        dqn_solver = DQNSolver(observation_space, action_space, exploration_decay,\n",
    "                               batch_size, learing_rate, gamma)\n",
    "\n",
    "        for run in self.dp.runs:\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state[0], [1, observation_space])\n",
    "            step = 0\n",
    "            for i in range(iterr):\n",
    "                step += 1\n",
    "                action = dqn_solver.act(state)\n",
    "                state_next, reward, terminal, info, _ = env.step(action)\n",
    "                if step % 30 == 0:\n",
    "                    print(\"Reward: {}\".format(reward))\n",
    "                state_next = np.reshape(state_next, [1, observation_space])\n",
    "                dqn_solver.remember(state, action, reward, state_next, terminal)\n",
    "                state = state_next\n",
    "                self.dp.add_reward_angle(reward, state[0][2])\n",
    "                if terminal:\n",
    "                    print(\"Run: {}, exploration: {}, score: {}\"\n",
    "                          .format(str(run),str(dqn_solver.exploration_rate), str(step)))\n",
    "                    print(\"Final Reward: {}\".format(reward))\n",
    "                    self.dp.append(dqn_solver.exploration_rate, step, reward)\n",
    "                    break\n",
    "                elif i == (iterr - 1):\n",
    "                    print(\"Iteration limit reached...\")\n",
    "                    print(\"Run: {}, exploration: {}, score: {}\"\n",
    "                          .format(str(run), str(dqn_solver.exploration_rate), str(step)))\n",
    "                    print(\"Final Reward: {}\".format(reward))\n",
    "                    self.dp.append(dqn_solver.exploration_rate, step, reward)\n",
    "                dqn_solver.experience_replay()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 20\n",
    "exploration_decay = 0.999\n",
    "learing_rate = 0.001\n",
    "gamma = 0.95\n",
    "episodes = 100\n",
    "iterr = 200\n",
    "\n",
    "# Initialize the Cartpole system\n",
    "cp = CartPole(episodes)\n",
    "\n",
    "# Start training\n",
    "cp.train(exploration_decay, batch_size, learing_rate, gamma, iterr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188daff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export csv file\n",
    "cp.dp.folder()\n",
    "cp.dp.toCSV()\n",
    "\n",
    "# Plotting\n",
    "cp.dp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952b57f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
